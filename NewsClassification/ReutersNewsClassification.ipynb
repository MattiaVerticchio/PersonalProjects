{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ReutersNewsClassification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattiaVerticchio/PersonalProjects/blob/master/NewsClassification/ReutersNewsClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxeWKevJKMkn",
        "colab_type": "text"
      },
      "source": [
        "# Reuters Dataset\n",
        "\n",
        "\n",
        "\n",
        "> **Abstract**\n",
        ">\n",
        "> \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtbLBrMJKR5l",
        "colab_type": "text"
      },
      "source": [
        "## Framework setup\n",
        "As central framework for this task we’ll use [Auto-Keras](https://arxiv.org/abs/1806.10282), an efficient neural architecture search system developed by DATA Lab at Texas A&M University. It leverages a variant of Bayesian Optimization to guide deep neural network morphism and find a good architecture for our task and dataset, using Keras and TensorFlow as backend.\n",
        "First, we have to install it with its dependency, [Keras-Tuner](https://keras-team.github.io/keras-tuner/), which is the hyperparameter optimization library used by Auto-Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB8AiNrBru5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "pip install -q git+https://github.com/keras-team/keras-tuner.git@1.0.2rc1\n",
        "pip install -q autokeras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iBvLGViKU9G",
        "colab_type": "text"
      },
      "source": [
        "## Imports\n",
        "Now we can import the TensorFlow dataset loading tool and Auto-Keras, as well as NumPy to store images and labels in arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUDTOvHb2HQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.datasets import reuters\n",
        "import autokeras as ak\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxtW-SBuKao1",
        "colab_type": "text"
      },
      "source": [
        "## Loading the data\n",
        "The dataset can be now loaded into four NumPy arrays. The data has an offset of 3 units, the first indeces, in fact are occupied as follows.\n",
        "\n",
        "0. Padding\n",
        "1. Start of sequence\n",
        "2. Unknown word\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPLZsQLx2LqR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "faf1f3d5-aa00-4455-ab43-335009eda389"
      },
      "source": [
        "offset = 3  # Index offset\n",
        "(x_train, y_train), (x_test, y_test) =  reuters.load_data(num_words=1000, \n",
        "                                                          index_from=offset)\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "y_test  = y_test.reshape(-1, 1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzWcDdsr76bO",
        "colab_type": "text"
      },
      "source": [
        "Let’s prepare the dictionary that maps indeces to words.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zauJ8DUhI9Wm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "13ec46f9-c253-4586-af9d-4492b7363be7"
      },
      "source": [
        "word_to_id = reuters.get_word_index()\n",
        "word_to_id = {k: (v + offset) for k, v in word_to_id.items()}\n",
        "word_to_id['PADDING'] = 0\n",
        "word_to_id['START_OF_SEQUENCE'] = 1\n",
        "word_to_id['UNKNKOWN'] = 2\n",
        "id_to_word = {value: key for key, value in word_to_id.items()}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters_word_index.json\n",
            "557056/550378 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srwfsoVz7wuX",
        "colab_type": "text"
      },
      "source": [
        "Now we convert the lists of indeces to actual words using a dictionary map. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kTM9qMKLav1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = list(map(lambda sentence: ' '.join(\n",
        "    id_to_word[i] for i in sentence), x_train))\n",
        "x_test = list(map(lambda sentence: ' '.join(\n",
        "    id_to_word[i] for i in sentence), x_test))\n",
        "\n",
        "x_train = np.array(x_train, dtype=np.str)\n",
        "x_test  = np.array(x_test, dtype=np.str)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzJyekMOKhve",
        "colab_type": "text"
      },
      "source": [
        "## Building the model\n",
        "`TextClassifier` is the class responsible for model search. Here I set `max_trials=3` to avoid taking too much time for model exploration, but it can be set to any positive integer. It explores different model architectures by tree-based Bayesian Optimization search. We’ll try only the first one as it’s really time consuming on Google Colab’s GPU, where I’m running this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_gZDMRf2z8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = ak.TextClassifier(              # Initialize the text classifier\n",
        "    num_classes=None,                 # Infer the number of classes\n",
        "    multi_label=False,                # Only one output\n",
        "    loss='categorical_crossentropy',  # Select the loss metric\n",
        "    metrics='accuracy',               # Metric to watch\n",
        "    project_name=\"text_classifier\",   # Name of the folder\n",
        "    max_trials=3,                     # Just try three models\n",
        "    directory=None,                   # Automatic folder creation\n",
        "    objective=\"val_loss\",             # Validation set crossentropy\n",
        "    tuner=None,                       # Automatic hyperparameter tuner selection\n",
        "    overwrite=True,                   # Don't load previous experiments\n",
        "    seed=42                           # Set a seed to replicate the experiment\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVTTD1ipKw9H",
        "colab_type": "text"
      },
      "source": [
        "### Neural Architecture Search\n",
        "It’s all ready to start exploring the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22dg4BVh25Yt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "6dd7719f-f381-4061-e103-bffd451cc9f3"
      },
      "source": [
        "clf.fit(                   # Fit the model\n",
        "    x=x_train,             # Training features\n",
        "    y=y_train,             # Training labels\n",
        "    epochs=None,           # Automatic number of epochs\n",
        "    callbacks=None,        # No callbacks\n",
        "    validation_split=0.2,  # Validation data split\n",
        "    validation_data=None   # Use a portion of training data\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 3 Complete [00h 01m 31s]\n",
            "val_loss: 1.2355173826217651\n",
            "\n",
            "Best val_loss So Far: 0.9400835037231445\n",
            "Total elapsed time: 00h 04m 27s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "Epoch 1/7\n",
            "281/281 [==============================] - 4s 13ms/step - loss: 2.0264 - accuracy: 0.4977\n",
            "Epoch 2/7\n",
            "281/281 [==============================] - 4s 13ms/step - loss: 1.3884 - accuracy: 0.6781\n",
            "Epoch 3/7\n",
            "281/281 [==============================] - 4s 13ms/step - loss: 1.1449 - accuracy: 0.7263\n",
            "Epoch 4/7\n",
            "281/281 [==============================] - 4s 13ms/step - loss: 0.9989 - accuracy: 0.7633\n",
            "Epoch 5/7\n",
            "281/281 [==============================] - 4s 13ms/step - loss: 0.8885 - accuracy: 0.7843\n",
            "Epoch 6/7\n",
            "281/281 [==============================] - 4s 13ms/step - loss: 0.7974 - accuracy: 0.8043\n",
            "Epoch 7/7\n",
            "281/281 [==============================] - 4s 13ms/step - loss: 0.7207 - accuracy: 0.8187\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bC2w_ARPK0a8",
        "colab_type": "text"
      },
      "source": [
        "The model found can now be exported, it’s a convolutional multi-layer neural network with the following architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15GDBnRaK3P0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "f35d649c-d024-4912-b4ef-afb0a4f7a8cb"
      },
      "source": [
        "model = clf.export_model()\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1)]               0         \n",
            "_________________________________________________________________\n",
            "text_vectorization (TextVect (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 512, 64)           320064    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512, 64)           0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 508, 256)          82176     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "re_lu (ReLU)                 (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 46)                11822     \n",
            "_________________________________________________________________\n",
            "classification_head_1 (Softm (None, 46)                0         \n",
            "=================================================================\n",
            "Total params: 479,854\n",
            "Trainable params: 479,854\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjgsmnXtK551",
        "colab_type": "text"
      },
      "source": [
        "## Testing the model\n",
        "The benchmark chosen for this dataset is the experiment reported on the book [Deep Learning with Python by Francois Chollet](https://www.manning.com/books/deep-learning-with-python). The testing crossentropy of the benchmark is ~0.96.\n",
        "\n",
        "Let’s now test our model on the holdout test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNRVdplb28dJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3d3486a6-f588-4664-effe-563b7872304b"
      },
      "source": [
        "# Evaluate the best model with testing data.\n",
        "current = clf.evaluate(x_test, y_test)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71/71 [==============================] - 0s 6ms/step - loss: 0.9296 - accuracy: 0.7854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FTBZ_w0UBCw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "613c662d-7f24-45c1-ace5-5eb2082e8a0d"
      },
      "source": [
        "previous = 0.9565213431445807\n",
        "improvement = (abs(current[0] - previous) / previous) * 100.0\n",
        "\n",
        "print(f'The categorical crossentropy improvement is {round(improvement, 1)}%.')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The categorical crossentropy improvement is 2.8%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BTStjjCK8On",
        "colab_type": "text"
      },
      "source": [
        "The accuracy of ~0.79 is in line with the previous model. However, we scored a cross entropy of ~0.93 with only three models explored, outperforming the textbook benchmark by almost 3%.\n",
        "\n",
        "Depending on our hardware and time availability, of course, we could explore even more models for further improvement in the benchmark score.\n",
        "\n",
        "[**Go back to index >**](https://github.com/MattiaVerticchio/PersonalProjects/blob/master/README_EN.md)"
      ]
    }
  ]
}