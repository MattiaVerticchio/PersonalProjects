{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Santander Customer Transaction Prediction.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNidupCs1+FqZe4zEB2YQm8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattiaVerticchio/PersonalProjects/blob/master/TransactionPrediction/TransactionPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lapcm9KsCZa"
      },
      "source": [
        "# Santander Customer Transaction Prediction\n",
        "> [Italiano]() / **English**\n",
        "\n",
        "> **Abstract**\n",
        ">\n",
        "> The objective of this notebook is to predict customer behavior. The problem is a binary classification, where we try to predict if a customer will (`1`) or won’t (`0`) make a transaction. The dataset contains 200 real features and one boolean target. The metric for evaluation is the Area Under the Receiver Operating Characteristic Curve (ROC-AUC)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GrS0OID0fnu"
      },
      "source": [
        "## Introduction\n",
        "To tune the classification model, we’ll use `optuna`, which is a hyperparameter optimization framework. The model we’ll train is Microsoft’s LightGBM, a gradient boosting decision tree learner, integrated with `optuna`. Let’s first install the packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAVNFbUi6hVf"
      },
      "source": [
        "%%bash\n",
        "pip install -q optuna"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZomBHtFgZtqw"
      },
      "source": [
        "Once installed, we’ll retrieve the dataset from the source. Here we’ll use Kaggle APIs to download the dataset from the Santander Customer Transaction Prediction competition as a `zip` file.\n",
        "\n",
        "The `JSON` file contains a unique individual `username` and `key`, retrievable from each Kaggle account settings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-kr9Bpc4EDA",
        "outputId": "ee400d84-e0b1-4759-f71b-ffda71e14384",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "%%bash\n",
        "# Set up Kaggle APIs\n",
        "mkdir ~/.kaggle/\n",
        "touch ~/.kaggle/kaggle.json\n",
        "chmod 600 ~/.kaggle/kaggle.json\n",
        "echo '{\"username\": \"mattiavert\", \"key\": \"875616a9d59f306292b1d150195cf075\"}' >> ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download the file\n",
        "kaggle competitions download -c santander-customer-transaction-prediction"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test.csv.zip to /content\n",
            "\n",
            "Downloading train.csv.zip to /content\n",
            "\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0.00/125M [00:00<?, ?B/s]\r  4%|4         | 5.00M/125M [00:00<00:03, 40.4MB/s]\r  9%|8         | 11.0M/125M [00:00<00:02, 44.9MB/s]\r 18%|#7        | 22.0M/125M [00:00<00:01, 54.5MB/s]\r 26%|##6       | 33.0M/125M [00:00<00:01, 64.2MB/s]\r 33%|###2      | 41.0M/125M [00:00<00:01, 66.8MB/s]\r 39%|###9      | 49.0M/125M [00:00<00:02, 39.6MB/s]\r 56%|#####6    | 70.0M/125M [00:01<00:01, 52.4MB/s]\r 65%|######4   | 81.0M/125M [00:01<00:00, 49.9MB/s]\r 78%|#######7  | 97.0M/125M [00:01<00:00, 61.3MB/s]\r 91%|######### | 113M/125M [00:01<00:00, 74.2MB/s] \r100%|##########| 125M/125M [00:01<00:00, 77.2MB/s]\n",
            "\r  0%|          | 0.00/125M [00:00<?, ?B/s]\r  5%|4         | 6.00M/125M [00:00<00:01, 62.5MB/s]\r 18%|#7        | 22.0M/125M [00:00<00:01, 76.8MB/s]\r 22%|##2       | 28.0M/125M [00:00<00:01, 68.8MB/s]\r 33%|###2      | 41.0M/125M [00:00<00:01, 49.5MB/s]\r 50%|####9     | 62.0M/125M [00:00<00:01, 64.4MB/s]\r 70%|######9   | 87.0M/125M [00:00<00:00, 83.1MB/s]\r 88%|########8 | 110M/125M [00:01<00:00, 103MB/s]  \r100%|##########| 125M/125M [00:01<00:00, 95.8MB/s]\n",
            "\r  0%|          | 0.00/462k [00:00<?, ?B/s]\r100%|##########| 462k/462k [00:00<00:00, 145MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhR4daX7aF8p"
      },
      "source": [
        "### Preprocessing\n",
        "Let’s import the installed libraries and Pandas to manage the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQnw8O016eXk"
      },
      "source": [
        "import pandas as pd                        # Data management\n",
        "import optuna.integration.lightgbm as lgb  # Hyperparameter optimization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWfVIieGShfE"
      },
      "source": [
        "Here we’ll read the dataset and separate features and target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCVAAN3-3rbx"
      },
      "source": [
        "X_train = pd.read_csv('train.csv.zip', index_col='ID_code')  # Training data\n",
        "X_test  = pd.read_csv('test.csv.zip',  index_col='ID_code')  # Testing data\n",
        "\n",
        "y_train = X_train[['target']].astype('bool')  # Separating features and target\n",
        "X_train = X_train.drop(columns='target')\n",
        "\n",
        "X = X_train.append(X_test)  # Matrix for all the features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxFE3J8KMT7j"
      },
      "source": [
        "On Google Colaboratory, we cannot widely explore feature augmentation with a dataset of this size. It could be useful to explore the following techniques:\n",
        "- Feature interaction\n",
        "- Feature ratio\n",
        "- Polynomial combinations\n",
        "- Trigonometric transforms\n",
        "- Clustering\n",
        "\n",
        "However, due to memory limits, I will only add a few new aggregated columns on the `X` DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9NH37dGJgJ0"
      },
      "source": [
        "cols = X.columns.values\n",
        "\n",
        "X['sum']  = X[cols].sum(axis=1)       # Sum of all the values\n",
        "X['min']  = X[cols].min(axis=1)       # Minimum value in the sample\n",
        "X['max']  = X[cols].max(axis=1)       # Maximum value in the sample\n",
        "X['mean'] = X[cols].mean(axis=1)      # Mean sample value\n",
        "X['std']  = X[cols].std(axis=1)       # Standard deviation of the sample\n",
        "X['var']  = X[cols].var(axis=1)       # Variance of the sample\n",
        "X['skew'] = X[cols].skew(axis=1)      # Skewness of the sample\n",
        "X['kurt'] = X[cols].kurtosis(axis=1)  # Kurtosis of each sample\n",
        "X['med']  = X[cols].median(axis=1)    # Median sample value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDgHyMjslVzu"
      },
      "source": [
        "Now let’s create the train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsnWQ3_976y3"
      },
      "source": [
        "dtrain = lgb.Dataset(X.iloc[0:200000], label=y_train)  # Training data\n",
        "X_test = X.iloc[200000:400000]                         # Testing data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p3KQlthVCxm"
      },
      "source": [
        "## Model building\n",
        "The learning model we’ll use is Microsoft’s LightGBM, a fast gradient boosting decision tree implementation, wrapped by `optuna`, as an optimizer for hyperparameters.\n",
        "\n",
        "The hyperparameters are optimized using a step wise process that follows a particular, well-established order:\n",
        "- `feature_fraction`\n",
        "- `num_leaves`\n",
        "- `bagging`\n",
        "- `feature_fraction` \n",
        "- `regularization_factors`\n",
        "- `min_data_in_leaf`\n",
        "\n",
        "Firstly, we define a few parameters for the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyD2HY5hG39C"
      },
      "source": [
        "# Dictionary of starting LightGBM parameters\n",
        "params = {\n",
        "    \"objective\": \"binary\",    # Binary classification\n",
        "    \"metric\": \"auc\",          # Used in competition\n",
        "    \"verbosity\": -1,          # Stay silent\n",
        "    \"boosting_type\": \"gbdt\",  # Gradient Boosting Decision Tree\n",
        "    \"max_bin\": 63,            # Faster training on GPU\n",
        "    \"num_threads\": 2,         # Use all physical cores of CPU\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7k09BhDUJ5G"
      },
      "source": [
        "Then we create a `LightGBMTunerCV` object. We perform a 5-Folds Stratified Cross Validation to check the accuracy of the model. I set a very high `num_boost_round` and enabled early training stopping to avoid overfitting on training data, since that could lead to poor generalization on unseen data. Patience for early stopping is set at 100 rounds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwMSVsGlvrx8",
        "outputId": "d457c86d-8c3f-4758-a243-179c373a7b88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Tuner object with Stratified 5-Fold Cross Validation\n",
        "tuner = lgb.LightGBMTunerCV(\n",
        "    params,                     # GBM settings\n",
        "    dtrain,                     # Training dataset\n",
        "    num_boost_round=999999,     # Set max iterations\n",
        "    nfold=5,                    # Number of CV folds\n",
        "    stratified=True,            # Stratified samples\n",
        "    early_stopping_rounds=100,  # Callback for CV's AUC\n",
        "    verbose_eval=False          # Stay silent\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[I 2020-09-26 13:40:18,787] A new study created in memory with name: no-name-2b82055c-7e7f-422a-9968-31cc3e0187c2\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCtU7kYFJUPD"
      },
      "source": [
        "### Hyperparameters tuning\n",
        "`optuna` provides calls to perform the search, let’s execute them in the established order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74KTxVmRObIW"
      },
      "source": [
        "tuner.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdMyWshj8uq0"
      },
      "source": [
        "Here are the results.\n",
        "- `feature_fraction` = 0.48\n",
        "- `num_leaves` = 3\n",
        "- `bagging_fraction` = 0.8662505913776934\n",
        "- `bagging_freq` = 7\n",
        "- `lambda_l1` = 2.6736262550429385e-08\n",
        "- `lambda_l2` = 0.0013546195528208944\n",
        "- `min_child_samples` = 50\n",
        "\n",
        "The next step is to find a good `num_boost_rounds` via cross-validation to retrain the final model without overfitting. Here I set the hyperparameters we found and start training with 10-Folds Stratified Cross-Validation with early stopping. This time the patience threshold is set to 1000, in this way we can be sure to reach the best model we can with this settings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-WVbmiKc7q0"
      },
      "source": [
        "# Dictionary of tuned LightGBM parameters\n",
        "params = {\n",
        "    \"objective\": \"binary\",    # Binary classification\n",
        "    \"metric\": \"auc\",          # Used in competition\n",
        "    \"verbosity\": -1,          # Stay silent\n",
        "    \"boosting_type\": \"gbdt\",  # Gradient Boosting Decision Tree\n",
        "    \"max_bin\": 63,            # Faster training on GPU\n",
        "    \"num_threads\": 2,         # Use all physical cores of CPU\n",
        "    # Adding optimizaed hyperparameters\n",
        "    \"feature_fraction\": 0.48,\n",
        "    \"num_leaves\": 3,\n",
        "    \"bagging_fraction\" : 0.8662505913776934,\n",
        "    \"bagging_freq\" : 7,\n",
        "    \"lambda_l1\": 2.6736262550429385e-08,\n",
        "    \"lambda_l2\": 0.0013546195528208944,\n",
        "    \"min_child_samples\": 50\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTsquNZkV6Xu"
      },
      "source": [
        "We now create and train the object with the found settings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22jRIiQPc-uE"
      },
      "source": [
        "finalModel = lgb.cv(\n",
        "    params,\n",
        "    dtrain,\n",
        "    num_boost_round=999999,\n",
        "    early_stopping_rounds=1000,\n",
        "    nfold=10,\n",
        "    stratified=True,\n",
        "    verbose_eval=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8dSYFQ9XRu1"
      },
      "source": [
        "At this point we can train the final model on the whole dataset, using the optimized hyperparameters and number of boosting rounds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ95a9Lv0pyd"
      },
      "source": [
        "# Importing the official library\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Retrieving the best training iteration\n",
        "CV_results = pd.DataFrame(finalModel)\n",
        "best_iterations = CV_results['auc-mean'].idxmax()\n",
        "\n",
        "# Training the final model \n",
        "model = lgb.train(params, dtrain, num_boost_round=best_iterations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtEqOLI1YKbL"
      },
      "source": [
        "# Results & Conclusions\n",
        "\n",
        "This particular experiment focused on hyperparameter tuning, but what could be done to furtherly improve the scores?\n",
        "\n",
        "Of course, we could dive deeper into feature engineering by augmenting the available data with the methods described above. Also, an ensemble learning model could be implemented to combine different model architectures and stack/blend the results."
      ]
    }
  ]
}