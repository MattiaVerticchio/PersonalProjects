{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Santander Customer Transaction Prediction.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMgRTFHrsFmKbeL6QA0tsGz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattiaVerticchio/PersonalProjects/blob/master/TransactionPrediction/TransactionPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lapcm9KsCZa"
      },
      "source": [
        "# Santander Customer Transaction Prediction\n",
        "> [Italiano]() / **English**\n",
        "\n",
        "> **Abstract**\n",
        ">\n",
        "> The objective of this notebook is to predict customer behavior. The problem is a binary classification, where we try to predict if a customer will (`1`) or won’t (`0`) make a transaction. The dataset contains 200 real anonymized features and one boolean target. We’ll use LightGBM as an ensemble learning model. The metric for evaluation is the Area Under the Receiver Operating Characteristic Curve (ROC-AUC), and the final cross-validated score is ~0.90."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GrS0OID0fnu"
      },
      "source": [
        "## Introduction\n",
        "To tune the classification model, we’ll use `optuna`, which is a hyperparameter optimization framework. The model we’ll train is Microsoft’s LightGBM, a gradient boosting decision tree learner, integrated with `optuna`. Let’s first install the packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAVNFbUi6hVf"
      },
      "source": [
        "%%bash\n",
        "pip install -q optuna"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZomBHtFgZtqw"
      },
      "source": [
        "Once installed, we’ll retrieve the dataset from the source. Here we’ll use Kaggle APIs to download the dataset from the Santander Customer Transaction Prediction competition as a `zip` file.\n",
        "\n",
        "The `JSON` file contains a unique individual `username` and `key`, retrievable from each Kaggle account settings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-kr9Bpc4EDA",
        "outputId": "8bf9869c-424f-4126-9bb3-09304c44fd46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "%%bash\n",
        "# Set up Kaggle APIs\n",
        "mkdir ~/.kaggle/\n",
        "touch ~/.kaggle/kaggle.json\n",
        "chmod 600 ~/.kaggle/kaggle.json\n",
        "echo '{\"username\": \"mattiavert\", \"key\": \"875616a9d59f306292b1d150195cf075\"}' >> ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download the file\n",
        "kaggle competitions download -c santander-customer-transaction-prediction"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading train.csv.zip to /content\n",
            "\n",
            "Downloading test.csv.zip to /content\n",
            "\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0.00/125M [00:00<?, ?B/s]\r  6%|6         | 8.00M/125M [00:00<00:01, 83.8MB/s]\r  9%|8         | 11.0M/125M [00:00<00:02, 42.7MB/s]\r 28%|##8       | 35.0M/125M [00:00<00:01, 56.8MB/s]\r 35%|###5      | 44.0M/125M [00:00<00:01, 51.7MB/s]\r 54%|#####3    | 67.0M/125M [00:00<00:00, 67.5MB/s]\r 63%|######3   | 79.0M/125M [00:00<00:00, 71.1MB/s]\r 78%|#######7  | 97.0M/125M [00:01<00:00, 78.8MB/s]\r 93%|#########2| 116M/125M [00:01<00:00, 95.3MB/s] \r100%|##########| 125M/125M [00:01<00:00, 106MB/s] \n",
            "\r  0%|          | 0.00/125M [00:00<?, ?B/s]\r  7%|7         | 9.00M/125M [00:00<00:01, 74.0MB/s]\r 19%|#9        | 24.0M/125M [00:00<00:01, 87.7MB/s]\r 33%|###2      | 41.0M/125M [00:00<00:01, 83.2MB/s]\r 47%|####6     | 58.0M/125M [00:00<00:00, 98.7MB/s]\r 59%|#####8    | 73.0M/125M [00:00<00:00, 91.6MB/s]\r 73%|#######2  | 91.0M/125M [00:00<00:00, 108MB/s] \r 84%|########4 | 105M/125M [00:00<00:00, 116MB/s] \r 95%|#########4| 118M/125M [00:01<00:00, 63.2MB/s]\r100%|##########| 125M/125M [00:01<00:00, 85.8MB/s]\n",
            "\r  0%|          | 0.00/462k [00:00<?, ?B/s]\r100%|##########| 462k/462k [00:00<00:00, 65.7MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhR4daX7aF8p"
      },
      "source": [
        "### Preprocessing\n",
        "Let’s import the installed libraries and Pandas to manage the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQnw8O016eXk"
      },
      "source": [
        "import pandas as pd                        # Data management\n",
        "import optuna.integration.lightgbm as lgb  # Hyperparameter optimization"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWfVIieGShfE"
      },
      "source": [
        "Here we’ll read the dataset and separate features and target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCVAAN3-3rbx"
      },
      "source": [
        "X_train = pd.read_csv('train.csv.zip', index_col='ID_code')  # Training data\n",
        "X_test  = pd.read_csv('test.csv.zip',  index_col='ID_code')  # Testing data\n",
        "\n",
        "y_train = X_train[['target']].astype('bool')  # Separating features and target\n",
        "X_train = X_train.drop(columns='target')\n",
        "\n",
        "X = X_train.append(X_test)  # Matrix for all the features"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxFE3J8KMT7j"
      },
      "source": [
        "On Google Colaboratory, we cannot widely explore feature augmentation with a dataset of this size. It could be useful to explore different techniques, however, due to memory limits, I will only add a few new aggregated columns on the `X` DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9NH37dGJgJ0"
      },
      "source": [
        "cols = X.columns.values\n",
        "\n",
        "X['sum']  = X[cols].sum(axis=1)       # Sum of all the values\n",
        "X['min']  = X[cols].min(axis=1)       # Minimum value in the sample\n",
        "X['max']  = X[cols].max(axis=1)       # Maximum value in the sample\n",
        "X['mean'] = X[cols].mean(axis=1)      # Mean sample value\n",
        "X['std']  = X[cols].std(axis=1)       # Standard deviation of the sample\n",
        "X['var']  = X[cols].var(axis=1)       # Variance of the sample\n",
        "X['skew'] = X[cols].skew(axis=1)      # Skewness of the sample\n",
        "X['kurt'] = X[cols].kurtosis(axis=1)  # Kurtosis of each sample\n",
        "X['med']  = X[cols].median(axis=1)    # Median sample value"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDgHyMjslVzu"
      },
      "source": [
        "Now let’s create the train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsnWQ3_976y3"
      },
      "source": [
        "dtrain = lgb.Dataset(X.iloc[0:200000], label=y_train)  # Training data\n",
        "X_test = X.iloc[200000:400000]                         # Testing data"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p3KQlthVCxm"
      },
      "source": [
        "## Model building\n",
        "The learning model we’ll use is Microsoft’s LightGBM, a fast gradient boosting decision tree implementation, wrapped by `optuna`, as an optimizer for hyperparameters.\n",
        "\n",
        "The hyperparameters are optimized using a step wise process that follows a particular, well-established order:\n",
        "- `feature_fraction`\n",
        "- `num_leaves`\n",
        "- `bagging`\n",
        "- `feature_fraction` \n",
        "- `regularization_factors`\n",
        "- `min_data_in_leaf`\n",
        "\n",
        "Firstly, we define a few parameters for the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyD2HY5hG39C"
      },
      "source": [
        "params = {                    # Dictionary of starting parameters\n",
        "    \"objective\": \"binary\",    # Binary classification\n",
        "    \"metric\": \"auc\",          # Used in competition\n",
        "    \"verbosity\": -1,          # Stay silent\n",
        "    \"boosting_type\": \"gbdt\",  # Gradient Boosting Decision Tree\n",
        "    \"max_bin\": 63,            # Faster training on GPU\n",
        "    \"num_threads\": 2,         # Use all physical cores of CPU\n",
        "}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7k09BhDUJ5G"
      },
      "source": [
        "Then we create a `LightGBMTunerCV` object. We perform a 5-Folds Stratified Cross Validation to check the accuracy of the model. I set a very high `num_boost_round` and enabled early training stopping to avoid overfitting on training data, since that could lead to poor generalization on unseen data. Patience for early stopping is set at 100 rounds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwMSVsGlvrx8",
        "outputId": "31ffec58-685c-4156-dcd7-a518e1632028",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "tuner = lgb.LightGBMTunerCV(    # Tuner object with Stratified 5-Fold CV\n",
        "    params,                     # GBM settings\n",
        "    dtrain,                     # Training dataset\n",
        "    num_boost_round=999999,     # Set max iterations\n",
        "    nfold=5,                    # Number of CV folds\n",
        "    stratified=True,            # Stratified samples\n",
        "    early_stopping_rounds=100,  # Callback for CV's AUC\n",
        "    verbose_eval=False          # Stay silent\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[I 2020-09-27 09:18:47,388] A new study created in memory with name: no-name-5d9e68ba-ed35-47ff-8c8c-73b9f862c708\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCtU7kYFJUPD"
      },
      "source": [
        "### Hyperparameters tuning\n",
        "`optuna` provides calls to perform the search, let’s execute them in the established order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74KTxVmRObIW"
      },
      "source": [
        "tuner.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdMyWshj8uq0"
      },
      "source": [
        "Here are the results.\n",
        "- `feature_fraction = 0.48` \n",
        "- `num_leaves = 3`\n",
        "- `bagging_fraction = 0.8662505913776934`\n",
        "- `bagging_freq = 7`\n",
        "- `lambda_l1 = 2.6736262550429385e-08`\n",
        "- `lambda_l2 = 0.0013546195528208944`\n",
        "- `min_child_samples = 50`\n",
        "\n",
        "The next step is to find a good `num_boost_rounds` via cross-validation to retrain the final model without overfitting. Here I set the hyperparameters we found and start training with 10-Folds Stratified Cross-Validation with early stopping. This time the patience threshold is set to 20."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-WVbmiKc7q0"
      },
      "source": [
        "# Dictionary of tuned LightGBM parameters\n",
        "params = {\n",
        "    \"objective\": \"binary\",    # Binary classification\n",
        "    \"metric\": \"auc\",          # Used in competition\n",
        "    \"verbosity\": -1,          # Stay silent\n",
        "    \"boosting_type\": \"gbdt\",  # Gradient Boosting Decision Tree\n",
        "    \"max_bin\": 63,            # Faster training on GPU\n",
        "    \"num_threads\": 2,         # Use all physical cores of CPU\n",
        "    # Adding optimized hyperparameters\n",
        "    \"feature_fraction\": 0.48,\n",
        "    \"num_leaves\": 3,\n",
        "    \"bagging_fraction\" : 0.8662505913776934,\n",
        "    \"bagging_freq\" : 7,\n",
        "    \"lambda_l1\": 2.6736262550429385e-08,\n",
        "    \"lambda_l2\": 0.0013546195528208944,\n",
        "    \"min_child_samples\": 50\n",
        "}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTsquNZkV6Xu"
      },
      "source": [
        "We now create and train the object with the found settings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22jRIiQPc-uE"
      },
      "source": [
        "finalModel = lgb.cv(           # Training the cross-validated model\n",
        "    params,                    # Loading the parameters\n",
        "    dtrain,                    # Training dataset\n",
        "    num_boost_round=999999,    # Setting a lot of boosting rounds\n",
        "    early_stopping_rounds=20,  # Stop training after 20 non-productive rounds\n",
        "    nfold=10,                  # Cross-validation folds\n",
        "    stratified=True,           # Stratified sampling\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtEqOLI1YKbL"
      },
      "source": [
        "## Results & Conclusions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyDFgvCm93rY",
        "outputId": "ab62c23e-e453-4815-bb10-3917d8eee5af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "CV_results = pd.DataFrame(finalModel)             # Saving iterations\n",
        "best_iteration = CV_results['auc-mean'].idxmax()  # Best iteration\n",
        "CV_results.loc[best_iteration]                    # Best CV ROC-AUC  "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "auc-mean    0.897531\n",
              "auc-stdv    0.002463\n",
              "Name: 1886, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TFhO_D3-w5D"
      },
      "source": [
        "The model scored ~0.90 as cross-validated metric for ROC-AUC.\n",
        "\n",
        "This particular experiment focused on hyperparameter tuning, but what could be done to furtherly improve the scores of the whole model?\n",
        "\n",
        "- Explore feature engineering by augmenting the available data with the methods described above.\n",
        "    - Feature interaction\n",
        "    - Feature ratio\n",
        "    - Polynomial combinations\n",
        "    - Trigonometric transforms\n",
        "    - Clustering\n",
        "- We could implement an ensemble learning model to combine different models and stack/blend the results.\n",
        "- Calibrate the model prediction probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA6cj6x-3CB8"
      },
      "source": [
        "### Finalize the model\n",
        "At this point, we can train the final model on the whole dataset, using the optimized hyperparameters and the number of boosting rounds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ95a9Lv0pyd"
      },
      "source": [
        "import lightgbm as lgb  # Importing the official Microsoft LightGBM\n",
        "\n",
        "model = lgb.train(                   # Training the final model \n",
        "    params,                          # Loading the parameters            \n",
        "    dtrain,                          # Training dataset\n",
        "    num_boost_round=best_iterations  # Setting boosting rounds\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfkkqxgS4rh3"
      },
      "source": [
        "### [Go back to index >](https://github.com/MattiaVerticchio/PersonalProjects/blob/master/README_EN.md)"
      ]
    }
  ]
}