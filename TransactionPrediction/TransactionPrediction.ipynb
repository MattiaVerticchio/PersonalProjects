{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Santander Customer Transaction Prediction.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNgM3I4YD7wmSCnS8KkuO0J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattiaVerticchio/PersonalProjects/blob/master/TransactionPrediction/TransactionPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lapcm9KsCZa"
      },
      "source": [
        "# Santander Customer Transaction Prediction\n",
        "> [Italiano]() / **English**\n",
        "\n",
        "> **Abstract**\n",
        ">\n",
        "> The objective of this notebook is to predict customer behavior. The problem is a binary classification, where we try to predict if a customer will (`1`) or won’t (`0`) make a transaction. The dataset contains 200 real features and one boolean target. The metric for evaluation is the Area Under the Receiver Operating Characteristic Curve (ROC-AUC)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GrS0OID0fnu"
      },
      "source": [
        "## Introduction\n",
        "To build and tune the model, we’ll use `optuna`, which is a hyperparameter optimization framework. The model we’ll train is Microsoft’s LightGBM, a gradient boosting decision tree learner, integrated with `optuna`. Let’s install the packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAVNFbUi6hVf"
      },
      "source": [
        "%%bash\n",
        "# Hyperparameter optimization framework\n",
        "pip install --quiet optuna"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZomBHtFgZtqw"
      },
      "source": [
        "Once installed, we’ll retrieve the dataset from the source. Here we’ll use Kaggle APIs to download the dataset from the Santander C Customer Transaction Prediction competition as a `zip` file.\n",
        "\n",
        "The `JSON` file contains a unique individual `username` and `key`, retrievable from each Kaggle account settings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-kr9Bpc4EDA",
        "outputId": "c98c493e-b139-46b7-8976-36d122e5892b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "%%bash\n",
        "# Set up Kaggle APIs\n",
        "mkdir ~/.kaggle/\n",
        "touch ~/.kaggle/kaggle.json\n",
        "chmod 600 ~/.kaggle/kaggle.json\n",
        "echo '{\"username\": \"mattiavert\", \"key\": \"875616a9d59f306292b1d150195cf075\"}' >> ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download the file\n",
        "kaggle competitions download -c santander-customer-transaction-prediction"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test.csv.zip to /content\n",
            "\n",
            "Downloading train.csv.zip to /content\n",
            "\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0.00/125M [00:00<?, ?B/s]\r  4%|4         | 5.00M/125M [00:00<00:10, 12.1MB/s]\r  7%|7         | 9.00M/125M [00:01<00:17, 7.12MB/s]\r 22%|##1       | 27.0M/125M [00:01<00:10, 10.0MB/s]\r 33%|###2      | 41.0M/125M [00:02<00:07, 12.4MB/s]\r 39%|###9      | 49.0M/125M [00:03<00:06, 11.5MB/s]\r 53%|#####2    | 66.0M/125M [00:03<00:03, 16.0MB/s]\r 59%|#####9    | 74.0M/125M [00:03<00:02, 18.4MB/s]\r 65%|######4   | 81.0M/125M [00:04<00:03, 15.1MB/s]\r 71%|#######1  | 89.0M/125M [00:04<00:02, 15.3MB/s]\r 87%|########6 | 108M/125M [00:04<00:00, 21.2MB/s] \r 97%|#########7| 121M/125M [00:05<00:00, 18.1MB/s]\r100%|##########| 125M/125M [00:05<00:00, 22.4MB/s]\n",
            "\r  0%|          | 0.00/125M [00:00<?, ?B/s]\r  4%|4         | 5.00M/125M [00:00<00:13, 9.59MB/s]\r  7%|7         | 9.00M/125M [00:01<00:16, 7.49MB/s]\r 18%|#8        | 23.0M/125M [00:01<00:10, 10.5MB/s]\r 26%|##6       | 33.0M/125M [00:01<00:07, 12.7MB/s]\r 33%|###2      | 41.0M/125M [00:03<00:08, 10.5MB/s]\r 46%|####5     | 57.0M/125M [00:03<00:04, 14.6MB/s]\r 58%|#####7    | 72.0M/125M [00:03<00:02, 20.1MB/s]\r 65%|######4   | 81.0M/125M [00:04<00:03, 13.5MB/s]\r 71%|#######1  | 89.0M/125M [00:04<00:02, 16.6MB/s]\r 86%|########6 | 108M/125M [00:04<00:00, 22.9MB/s] \r 94%|#########4| 118M/125M [00:05<00:00, 26.0MB/s]\r100%|##########| 125M/125M [00:05<00:00, 24.8MB/s]\n",
            "\r  0%|          | 0.00/462k [00:00<?, ?B/s]\r100%|##########| 462k/462k [00:00<00:00, 63.8MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhR4daX7aF8p"
      },
      "source": [
        "### Preprocessing\n",
        "Let’s import the installed libraries and Pandas to manage the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQnw8O016eXk"
      },
      "source": [
        "# Data management\n",
        "import pandas as pd\n",
        "# Microsoft LightGBM classifier with hyperparameter optimization\n",
        "import optuna.integration.lightgbm as lgb"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWfVIieGShfE"
      },
      "source": [
        "Here we’ll read the dataset and separate features and target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCVAAN3-3rbx"
      },
      "source": [
        "# Reading train and test data\n",
        "X_train = pd.read_csv('train.csv.zip', index_col='ID_code')\n",
        "X_test  = pd.read_csv('test.csv.zip',  index_col='ID_code')\n",
        "\n",
        "# Separating features and target\n",
        "y_train = X_train[['target']].astype('bool')\n",
        "X_train = X_train.drop(columns='target')\n",
        "\n",
        "# Matrix for all the features\n",
        "X = X_train.append(X_test)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxFE3J8KMT7j"
      },
      "source": [
        "On Google Colaboratory, we cannot widely explore feature augmentation with a dataset of this size. It could be useful to explore the following techniques:\n",
        "- Feature interaction\n",
        "- Feature ratio\n",
        "- Polynomial combinations\n",
        "- Trigonometric transforms\n",
        "- Clustering\n",
        "\n",
        "However, due to memory limits, I will only add a few new aggregated columns on the `X` DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9NH37dGJgJ0"
      },
      "source": [
        "cols = X.columns.values\n",
        "\n",
        "X['sum']  = X[cols].sum(axis=1)\n",
        "X['min']  = X[cols].min(axis=1)\n",
        "X['max']  = X[cols].max(axis=1)\n",
        "X['mean'] = X[cols].mean(axis=1)\n",
        "X['std']  = X[cols].std(axis=1)\n",
        "X['var']  = X[cols].var(axis=1)\n",
        "X['skew'] = X[cols].skew(axis=1)\n",
        "X['kurt'] = X[cols].kurtosis(axis=1)\n",
        "X['med']  = X[cols].median(axis=1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDgHyMjslVzu"
      },
      "source": [
        "Now let’s create the train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsnWQ3_976y3"
      },
      "source": [
        "# Training LightGBM dataset\n",
        "dtrain = lgb.Dataset(X.iloc[0:200000], label=y_train)\n",
        "# Testing DataFrame\n",
        "X_test = X.iloc[200000:400000]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p3KQlthVCxm"
      },
      "source": [
        "## Model building\n",
        "The learning model we’ll use is Microsoft’s LightGBM, a fast gradient boosting decision tree implementation, wrapped by `optuna`, as an optimizer for hyperparameters.\n",
        "\n",
        "The hyperparameters are optimized using a step wise process that follows a particular, well-established order:\n",
        "- `feature_fraction`\n",
        "- `num_leaves`\n",
        "- `bagging`\n",
        "- `feature_fraction` \n",
        "- `regularization_factors`\n",
        "- `min_data_in_leaf`\n",
        "\n",
        "Firstly, we define a few parameters for the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyD2HY5hG39C"
      },
      "source": [
        "# Dictionary of starting LightGBM parameters\n",
        "params = {\n",
        "    \"objective\": \"binary\",    # Binary classification\n",
        "    \"metric\": \"auc\",          # Used in competition\n",
        "    \"verbosity\": -1,          # Stay silent\n",
        "    \"boosting_type\": \"gbdt\",  # Gradient Boosting Decision Tree\n",
        "    \"max_bin\": 63,            # Faster training on GPU\n",
        "    \"num_threads\": 2,         # Use all physical cores of CPU\n",
        "    }"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7k09BhDUJ5G"
      },
      "source": [
        "Then we create a `LightGBMTunerCV` object. We perform a 5-Folds Stratified Cross Validation to check the accuracy of the model. I set a very high `num_boost_round` and enabled early training stopping to avoid overfitting on training data, since that could lead to poor generalization on unseen data. Patience for early stopping is set at 100 rounds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwMSVsGlvrx8",
        "outputId": "d457c86d-8c3f-4758-a243-179c373a7b88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Tuner object with Stratified 5-Fold Cross Validation\n",
        "tuner = lgb.LightGBMTunerCV(params,                     # GBM settings\n",
        "                            dtrain,                     # Training dataset\n",
        "                            num_boost_round=999999,     # Set max iterations\n",
        "                            nfold=5,                    # Number of CV folds\n",
        "                            stratified=True,            # Stratified samples\n",
        "                            early_stopping_rounds=100,  # Callback for CV's AUC\n",
        "                            verbose_eval=False)         # Stay silent"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[I 2020-09-26 13:40:18,787] A new study created in memory with name: no-name-2b82055c-7e7f-422a-9968-31cc3e0187c2\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCtU7kYFJUPD"
      },
      "source": [
        "### Hyperparameters tuning\n",
        "`optuna` provides calls to perform the search, let’s execute them in the established order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74KTxVmRObIW"
      },
      "source": [
        "tuner.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdMyWshj8uq0"
      },
      "source": [
        "Here are the results.\n",
        "- `feature_fraction` = 0.48\n",
        "- `num_leaves` = 3\n",
        "- `bagging_fraction` = 0.8662505913776934\n",
        "- `bagging_freq` = 7\n",
        "- `lambda_l1` = 2.6736262550429385e-08\n",
        "- `lambda_l2` = 0.0013546195528208944\n",
        "- `min_child_samples` = 50\n",
        "\n",
        "The next step is to find a good `num_boost_rounds` via cross-validation to retrain the final model without overfitting. Here I set the hyperparameters we found and start training with 10-Folds Stratified Cross-Validation with early stopping. This time the patience threshold is set to 1000, in this way we can be sure to reach the best model we can with this settings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-WVbmiKc7q0"
      },
      "source": [
        "# Dictionary of tuned LightGBM parameters\n",
        "params = {\n",
        "    \"objective\": \"binary\",    # Binary classification\n",
        "    \"metric\": \"auc\",          # Used in competition\n",
        "    \"verbosity\": -1,          # Stay silent\n",
        "    \"boosting_type\": \"gbdt\",  # Gradient Boosting Decision Tree\n",
        "    \"max_bin\": 63,            # Faster training on GPU\n",
        "    \"num_threads\": 2,         # Use all physical cores of CPU\n",
        "    # Adding optimizaed hyperparameters\n",
        "    \"feature_fraction\": 0.48,\n",
        "    \"num_leaves\": 3,\n",
        "    \"bagging_fraction\" : 0.8662505913776934,\n",
        "    \"bagging_freq\" : 7,\n",
        "    \"lambda_l1\": 2.6736262550429385e-08,\n",
        "    \"lambda_l2\": 0.0013546195528208944,\n",
        "    \"min_child_samples\": 50}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTsquNZkV6Xu"
      },
      "source": [
        "We now create and train the object with the found settings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22jRIiQPc-uE"
      },
      "source": [
        "finalModel = lgb.cv(params,\n",
        "                    dtrain,\n",
        "                    num_boost_round=999999,\n",
        "                    early_stopping_rounds=1000,\n",
        "                    nfold=10,\n",
        "                    stratified=True,\n",
        "                    verbose_eval=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8dSYFQ9XRu1"
      },
      "source": [
        "At this point we can train the final model on the whole dataset, using the optimized hyperparameters and number of boosting rounds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ95a9Lv0pyd"
      },
      "source": [
        "# Importing the official library\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Retrieving the best training iteration\n",
        "CV_results = pd.DataFrame(finalModel)\n",
        "best_iterations = CV_results['auc-mean'].idxmax()\n",
        "\n",
        "# Training the final model \n",
        "model = lgb.train(params, dtrain, num_boost_round=best_iterations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abz2QyZwXdRy"
      },
      "source": [
        "With the final model, we can make the predictions on the test set and create a CSV file to submit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-ksSt41d6nw",
        "outputId": "341935e2-4066-4d28-9e4e-910b67569b3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        }
      },
      "source": [
        "pred = model.predict(X_test)\n",
        "df = pd.DataFrame(pred, columns=['target'])\n",
        "df.index.name = 'ID_code'\n",
        "df = df.rename('test_{}'.format)\n",
        "df.to_csv('sub.csv')\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ID_code</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>test_0</th>\n",
              "      <td>0.051829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test_1</th>\n",
              "      <td>0.206173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test_2</th>\n",
              "      <td>0.219142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test_3</th>\n",
              "      <td>0.253446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test_4</th>\n",
              "      <td>0.039699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test_199995</th>\n",
              "      <td>0.032182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test_199996</th>\n",
              "      <td>0.007454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test_199997</th>\n",
              "      <td>0.003174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test_199998</th>\n",
              "      <td>0.117466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test_199999</th>\n",
              "      <td>0.086354</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200000 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               target\n",
              "ID_code              \n",
              "test_0       0.051829\n",
              "test_1       0.206173\n",
              "test_2       0.219142\n",
              "test_3       0.253446\n",
              "test_4       0.039699\n",
              "...               ...\n",
              "test_199995  0.032182\n",
              "test_199996  0.007454\n",
              "test_199997  0.003174\n",
              "test_199998  0.117466\n",
              "test_199999  0.086354\n",
              "\n",
              "[200000 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edY0R8hiXz6F"
      },
      "source": [
        "As stated, using Kaggle APIs we submit the CSV and find out the AOC score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB8chI28dYam",
        "outputId": "70a2fee4-a3f2-4725-80c6-a35b0e8e8646",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "! kaggle competitions submit -c santander-customer-transaction-prediction -f /content/sub.csv -m Tuned_LightGBM"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100% 6.06M/6.06M [00:04<00:00, 1.39MB/s]\n",
            "Successfully submitted to Santander Customer Transaction Prediction"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtEqOLI1YKbL"
      },
      "source": [
        "# Results & Conclusions\n",
        "The results are the following:\n",
        "- Private score = 0.89610\n",
        "- Public score = 0.89867\n",
        "\n",
        "Overall an AOC score of ~0.90 for a single model prediction is not bad, considering that the top-5 that won the prize is placed at ~0.92.\n",
        "\n",
        "This particular experiment focused on hyperparameter tuning, but what could be done to furtherly improve the scores?\n",
        "\n",
        "Of course, we could dive deeper into feature engineering by augmenting the available data with the methods described above. Also, an ensemble learning model could be implemented to combine different model architectures and stack/blend the results."
      ]
    }
  ]
}